{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os \n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    os.chdir('..')\n",
    "import glob\n",
    "\n",
    "import sys\n",
    "print(sys.path)\n",
    "\n",
    "from sglm import utils, glm_fit\n",
    "\n",
    "#d1 - T430\n",
    "#d2 - T434"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, let's create a new project. The project directory will create a data and results folder and a config file.\n",
    "\n",
    "#### You will need to edit the config file with the particular glm params you wish to use. Fields that are necessary to edit are: predictors, predictors_shift_bounds, response, and the glm_keyword_args.\n",
    "\n",
    "#### You will also need to move your data into the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'D1_h3Hist_all_glm' \n",
    "project_dir = r'\\\\research.files.med.harvard.edu\\\\Neurobio\\\\MICROSCOPE\\\\Livia\\\\glm_output' # windows\n",
    "# project_dir = r'/Volumes/Neurobio/MICROSCOPE/Livia/glm_output' # mac\n",
    "\n",
    "utils.create_new_project(project_name, project_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Format Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input data should conform to the following convention and be saved as a *.csv:\n",
    "\n",
    "Indices / Unique Row Identifiers:\n",
    "* SessionName -- Any order is acceptable\n",
    "* TrialNumber-- Must be in chronological order, but does not need to start from zero\n",
    "* Timestamp -- Must be in chronological order, but does not need to start from zero\n",
    "\n",
    "Columns (Predictors + Responses):\n",
    "* Predictors - binary\n",
    "* Reponses - e.g. neural responses (analog or binary)\n",
    "\n",
    "Example, shown below is dummy data depicting a trial_0 that last four response timestamps:\n",
    "| SessionName | TrialNumber | Timestamp | predictor_1 | predictor_2 | predictor_3 | response_1 | response_2 |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| session_0 | trial_0 | -1 | 0 | 0 | 0 | 1 | 0.3 |\n",
    "| session_0 | trial_0 | 0 | 0 | 0 | 0 | 0 | 1.4 |\n",
    "| session_0 | trial_0 | 1 | 0 | 0 | 0 | 1 | 2.3 |\n",
    "| session_0 | trial_0 | 2 | 0 | 1 | 0 | 1 | 0.3 |\n",
    "| session_0 | trial_1 | -2 | 0 | 0 | 0 | 0 | 1.4 |\n",
    "| session_0 | trial_1 | -1 | 0 | 0 | 0 | 1 | 2.3 |\n",
    "| session_0 | trial_1 | 0 | 1 | 0 | 0 | 0 | 1.4 |\n",
    "| session_0 | trial_1 | 1 | 0 | 0 | 0 | 1 | 2.3 |\n",
    "| session_1 | trial_0 | 5 | 0 | 0 | 0 | 0 | 1.4 |\n",
    "| session_1 | trial_0 | 6 | 1 | 0 | 0 | 1 | 2.3 |\n",
    "| session_1 | trial_0 | 7 | 0 | 0 | 0 | 0 | 1.4 |\n",
    "| session_1 | trial_0 | 8 | 0 | 0 | 0 | 1 | 2.3 |\n",
    "| session_1 | trial_1 | 9 | 0 | 0 | 0 | 0 | 1.4 |\n",
    "| session_1 | trial_1 | 10 | 0 | 0 | 0 | 1 | 2.3 |\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's get set up to start our project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = os.path.join(project_dir, project_name)\n",
    "files = os.listdir(project_path)\n",
    "\n",
    "assert 'data' in files, 'data folder not found! {}'.format(files)\n",
    "assert 'results' in files, 'results folder not found! {}'.format(files)\n",
    "assert 'config.yaml' in files, 'config.yaml not found! {}'.format(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If needed, use the following function to combine multiple sessions into one csv. You will need a filename you wish to call your output_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = False\n",
    "if left:\n",
    "    # output_csv = 'output.csv'\n",
    "    p_split = project_name.split(\"_\")\n",
    "    p_name = p_split[0][:len(p_split[0])-1]\n",
    "    name = p_name +\"_\"+ p_split[1]\n",
    "    print(name)\n",
    "    output_csv = name +'Format.csv'\n",
    "else: output_csv = project_name +'Format.csv'\n",
    "\n",
    "output_csv\n",
    "# utils.combine_csvs(project_path, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we'll open the data and set the columns you wish to use as fixed indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = os.path.join(project_path, 'data', output_csv)\n",
    "index_col = ['SessionName', 'TrialNumber', 'Timestamp']\n",
    "\n",
    "df = utils.read_data(input_file, index_col)\n",
    "\n",
    "print('Your dataframe has {} rows and {} columns'.format(df.shape[0], df.shape[1]))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can now explore and add to the dataframe. As an example, you may want to add various \"predictors\" or \"features\" to explore. You can use the example below as inspiration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Identify the individual licks that have specific meaning in the tasks: \n",
    "# #lick 1, lick 2 and lick 3 are \"operant licks\" on different training days\n",
    "# #licknon1-3 are all the other licks\n",
    "\n",
    "# df_source = df.copy()\n",
    "# srs_lick = df_source.groupby(['SessionName', 'TrialNumber'])['Lick'].cumsum()\n",
    "# srs_lick_count = srs_lick * df_source['Lick']\n",
    "# df_lick_count_dummies = pd.get_dummies(srs_lick_count).drop(0, axis=1)\n",
    "# df_lick_count_dummies = df_lick_count_dummies[[1,2,3]]\n",
    "# df_lick_count_dummies['non1-3'] = df_source['Lick'] - df_lick_count_dummies.sum(axis=1)\n",
    "# df_lick_count_dummies.columns = [f'lick_{original_column_name}' for original_column_name in df_lick_count_dummies.columns]\n",
    "\n",
    "# # Columns lick and lick_1, lick_2, lick_3, lick_non-13 should not all be used together\n",
    "# # as predictors because of multicollinearity.\n",
    "# df_source = pd.concat([df_source, df_lick_count_dummies], axis=1)\n",
    "# df_source\n",
    "\n",
    "# assert np.all(df_source['Lick'] == df_source[['lick_1', 'lick_2', 'lick_3', 'lick_non1-3']].sum(axis=1)), 'Column lick should equal the sum of all other lick columns.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Friendly reminder, the df we have imported is mutli-index, meaning, it's organization is dependent on 3-columns that we have set in index_col. Therefore, we can use \"groupby\" if you need to split the organization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reIndex = df_source.groupby(level=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load your fitting paramaters and set up your train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = os.path.join(project_path, 'config.yaml')\n",
    "config = utils.load_config(config_file)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shift responses and predictors. If you do not want to shift your predictors by an amount you set, feel free to comment out the entire \"predictors_shift_bounds\" in config.yaml. We will then use the default set when we created the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(col, df[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_shift, df_predictors_shift, shifted_params = glm_fit.shift_predictors(config, df)\n",
    "print('Your dataframe was shifted using: {}'.format(shifted_params))\n",
    "\n",
    "df_predictors_shift.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictors_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enl_columns = df_predictors_shift.filter(like='enl', axis=1)\n",
    "session = 'T434_2023_08_25'\n",
    "filt_ind = [ind for ind, col in enumerate(enl_columns.index) if session in col]\n",
    "\n",
    "rows_taken = [col for col in enl_columns.index if session in col]\n",
    "print(F\"{session} rows num: {len(rows_taken)}\")\n",
    "\n",
    "combined_all = enl_columns.iloc[filt_ind].sum(axis=1)\n",
    "for com in combined_all.unique():\n",
    "    print(f\"{com} number: {len(np.where(combined_all == com)[0])}\")\n",
    "combined_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{0} number: {np.where(combined_all == 0)[0]}\")\n",
    "\n",
    "test = enl_columns.iloc[80:100]\n",
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create sparse array - by row\n",
    "import scipy\n",
    "sparse_df = scipy.sparse.csr_array(df_predictors_shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_shift \n",
    "temp = response_shift.values.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your test/train datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train,X_test, y_train, y_test = glm_fit.split_data(df_predictors_shift, temp, config) #changed from df_predictors_shift to sparse_df! \n",
    "X_train,X_test, y_train, y_test = glm_fit.split_data(sparse_df, temp, config) #changed from df_predictors_shift to sparse_df! \n",
    "\n",
    "print('Training data has {} rows and {} columns'.format(X_train.shape[0], X_train.shape[1]))\n",
    "print('Testing data has {} rows and {} columns'.format(X_test.shape[0], X_test.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test1 = (y_test.values).flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we're ready to run our GLM!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have two different options. If you know which params you would like to use, you can use the glm_fit.fit_glm function. If you would like tune your hyperparams to determine which are the best to use, you can use the glm_fit.fit_tuned_glm function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit the model\n",
    "model, y_pred, score, beta, intercept, sparse_beta = glm_fit.fit_glm(config, X_train, X_test, y_train, y_test)\n",
    "print('Your model can account for {} percent of your data'.format(score*100))\n",
    "modeltype = \"fit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shifts = config['glm_params']['predictors_shift_bounds']\n",
    "# print(df.columns)\n",
    "# ind = []\n",
    "# for col in df.columns:\n",
    "#     print(col)\n",
    "#     print(shifts[col])\n",
    "\n",
    "# test_pd = pd.DataFrame(X_test.toarray(), columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model with cross validation: remember, your alphas and l1_ratios should be lists\n",
    "# tuned_model, y_pred, score, beta, best_params = glm_fit.fit_tuned_glm(config, X_train, X_test, y_train, y_test)\n",
    "# print('Your model can account for {} percent of your data'.format(score*100))\n",
    "# modeltype = 'tuned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
=======
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Ridge model\n",
    "# ridge_model, y_pred, score, beta, intercept = glm_fit.fit_ridge(config, X_train, X_test, y_train, y_test)\n",
    "# print('Your model can account for {} percent of your data'.format(score*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Ridge model with cross validation: remember, your alphas should be a list\n",
    "# tuned_ridge_model, y_pred, score, beta, best_params = glm_fit.fit_tuned_ridge(config, X_train, X_test, y_train, y_test)\n",
    "# print('Your model can account for {} percent of your data'.format(score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
>>>>>>> bd1714ac0517bea010c5b30aa84ee8b49f17fefb
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, window):\n",
    "    \n",
    "    r = x.rolling(window=window,center=True)\n",
    "    \n",
    "    lower_bound = r.min()\n",
    "    upper_bound = r.max()\n",
    "    #lower_bound = r.quantile(0.05) # instead of .min()\n",
    "    #upper_bound = r.quantile(0.95) # instead of .max()\n",
    "    \n",
    "    return (x-lower_bound)/(upper_bound-lower_bound)\n",
    "  \n",
    "def plot_spikes_with_prediction(df, spikes, predicted_spikes, dt,\n",
    "                                nt=200, t0=120, **kws):\n",
    "  \"\"\"Plot actual and predicted spike counts.\n",
    "\n",
    "  Args:\n",
    "    spikes (1D array): Vector of actual spike counts\n",
    "    predicted_spikes (1D array): Vector of predicted spike counts\n",
    "    dt (number): Duration of each time bin.\n",
    "    nt (number): Number of time bins to plot\n",
    "    t0 (number): Index of first time bin to plot.\n",
    "    show (boolean): To plt.show or not the plot.\n",
    "    kws: Pass additional keyword arguments to plot()\n",
    "\n",
    "  \"\"\"\n",
    "  t = np.arange(t0, t0 + nt) * dt\n",
    "\n",
    "  f, ax = plt.subplots(2, figsize=(12, 25))\n",
    "  # lines = ax.stem(t, spikes[:nt])\n",
    "  # plt.setp(lines, color=\".5\")\n",
    "  \n",
    "  i=0\n",
    "  inc_height=1.2\n",
    "  window_start = 10200 \n",
    "  window_stop = 11000\n",
    "\n",
    "  cols_to_plot = [col for col in df.columns]\n",
    "  enl_columns = [col for col in df.columns if 'enl' in col]\n",
    "  not_enl = [col for col in cols_to_plot if not any(enl_col in col for enl_col in enl_columns)]\n",
    "  \n",
    "  for j, col in enumerate(not_enl):\n",
    "    if 'z_grn' in col:\n",
    "      ax[0].plot((df[col].values*0.2)[window_start:window_stop] + i, label=col)\n",
    "    else:\n",
    "      ax[0].plot((df[col].values)[window_start:window_stop] + i, label=col)\n",
    "    i += inc_height\n",
    "    ax[0].text(x=900, y=i - inc_height, s=col, fontsize=14, va=\"bottom\")\n",
    "  ax[0].set(\n",
    "      # xlabel=\"Bins\",\n",
    "      ylabel=\"Events\",\n",
    "  )\n",
    "  \n",
    " \n",
    "  print(df.index[window_start][1], df.index[window_stop][1])   \n",
    "  \n",
    "  t = np.arange(0, len(spikes))*dt\n",
    "  start = np.argmin(abs(t - df.index[window_start][1]))\n",
    "  stop = np.argmin(abs(t - df.index[window_stop][1]))\n",
    "  print(start, stop)\n",
    "  print(t[start], t[stop])\n",
    "  ax[1].plot(t[start:stop], spikes[start:stop], label='spikes')\n",
    "  # lines[-1].set_zorder(1)\n",
    "  kws.setdefault(\"linewidth\", 3)\n",
    "  yhat, = ax[1].plot(t[start:stop], predicted_spikes[start:stop], **kws, label='pred spikes')\n",
    "  ax[1].set(\n",
    "      xlabel=\"Bins\",\n",
    "      ylabel=\"Spikes\",\n",
    "  )\n",
    "  ax[1].yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "  ax[1].axhline(y=0, color='grey', linestyle=\"--\")\n",
    "  # ax.legend([lines[0], yhat], [\"Spikes\", \"Predicted\"])\n",
    "  ax[1].legend()\n",
    "  \n",
    "\n",
    "  print(len(np.arange(window_start,window_stop)), len(predicted_spikes[start:stop]))\n",
    "  plt.show()\n",
    "\n",
    "fs = 1/48.440600198412696\n",
    "\n",
    "\n",
    "# plot_spikes_with_prediction(y_test, y_pred, df.Timestamp[1] - df.Timestamp[0])\n",
    "plot_spikes_with_prediction(X_test, y_test, y_pred, fs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test.shape, y_pred.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "acc:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create your model dictonary, this should include all the information you wish to save\n",
    "if modeltype == \"fit\":\n",
    "    model_dict = {'model': model,\n",
    "              'y_pred': y_pred,\n",
    "              'score': score,\n",
    "              'beta': beta,\n",
    "              'intercept': intercept,\n",
    "              'sparse_beta': sparse_beta,}\n",
    "if modeltype == 'tuned':\n",
    "    model_dict = {'model': tuned_model,\n",
    "              'y_pred': y_pred,\n",
    "              'score': score,\n",
    "              'beta': beta,\n",
    "              'best_params': best_params,}\n",
    "    \n",
    "\n",
    "#Save your model dictionary\n",
    "model_path = project_path + '/models'\n",
    "model_name = project_name + '_model.pkl'\n",
    "model_full_path = os.path.join(model_path, model_name)\n",
    "import pickle\n",
    "with open(model_full_path, 'wb') as f:\n",
    "    pickle.dump(model_dict, f)\n",
    "        \n",
    "# glm_fit.save_model(model_dict, project_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and save figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_img = os.path.join(project_path)\n",
    "\n",
    "glm_fit.plot_and_save(save_img, config, y_pred, y_test, beta, df_predictors_shift, format(score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_enl = False\n",
    "if combine_enl:\n",
    "    #save model fit results\n",
    "    model_fit_results = pd.Series(beta, index=df_predictors_shift.columns, name='coef').unstack(0, )\n",
    "    model_fit_results.index = model_fit_results.index.astype(int)\n",
    "    model_fit_results = model_fit_results.reindex(config['glm_params']['predictors'], axis=1)\n",
    "\n",
    "    # new added by liv for enl appending:\n",
    "    if any('enl' in col for col in model_fit_results.columns):\n",
    "        # could add sorting for enl precursors ie. history?\n",
    "        enl_columns = [col for col in model_fit_results.columns if 'enl' in col]\n",
    "        enl_col_df = model_fit_results[enl_columns]\n",
    "        print(enl_col_df.shape)\n",
    "        # for icol, col in enumerate(enl_col_df.columns):\n",
    "            # print(\"col shape: \" + col.shape)\n",
    "            # print(f\"col ind: {icol}\")\n",
    "            # print(f\"col index pred shape: {enl_col_df.sort_index()[col]}\")\n",
    "        print(\"test - true: enl found\")\n",
    "            \n",
    "        enl_all = np.array([])\n",
    "        test = []\n",
    "        for icol, col in enumerate(enl_col_df.columns):\n",
    "            test.append(enl_col_df.sort_index()[col])\n",
    "            nans = np.isnan(enl_col_df.sort_index()[col])\n",
    "            enl_all = np.append(enl_all, enl_col_df.sort_index()[col][~nans])\n",
    "            \n",
    "        print(f\"enl all shape: {enl_all.shape}\")\n",
    "        enl_df = pd.DataFrame({'enl': enl_all})\n",
    "        \n",
    "        test = np.where(enl_df['enl'] == 0)[0]\n",
    "        model_test = model_fit_results.copy()\n",
    "        # print(model_test.index)\n",
    "        loc_insert = int((np.where(model_test.columns == enl_columns[0])[0])[0])\n",
    "        print(loc_insert)\n",
    "        \n",
    "        model_test = model_test.sort_index()\n",
    "        model_test = model_test.reindex(range(np.min(model_test.index), len(enl_all)))\n",
    "        # print(model_test.shape)\n",
    "        print(f\"model test shape: {model_test.shape}\")\n",
    "            \n",
    "        for col in enl_columns:\n",
    "            # print(f\"drop col: {str(col)}\")\n",
    "            model_test = model_test.drop(str(col), axis=1)\n",
    "\n",
    "        # print(model_test.index)\n",
    "        model_test = model_test.join(enl_df, how='left')\n",
    "        col = model_test.pop('enl')\n",
    "        model_test.insert(loc_insert, col.name, col)\n",
    "\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print(\"no enl found\")\n",
    "\n",
    "    ''' uncomment for comparison with and without joining enl:\n",
    "    tup_y_lim = (np.inf, -np.inf)\n",
    "    fig, axes = plt.subplots(1, len(model_fit_results.columns), figsize=(5*len(model_fit_results.columns), 5))\n",
    "    axes = axes.flatten()\n",
    "    for ipredictor, predictor in enumerate(model_fit_results.columns):\n",
    "            \n",
    "        axes[ipredictor].plot(model_fit_results.sort_index()[predictor])\n",
    "        axes[ipredictor].set_title(predictor)\n",
    "        axes[ipredictor].grid(True)\n",
    "        \n",
    "        tup_y_lim = (min(tup_y_lim[0], model_fit_results[predictor].min()-0.1),\n",
    "                    max(tup_y_lim[1], model_fit_results[predictor].max()+0.1))\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_ylim(tup_y_lim)\n",
    "    fig.suptitle('GLM Coefficients Fit Results')\n",
    "    '''\n",
    "    ###########\n",
    "    tup_y_lim = (np.inf, -np.inf)\n",
    "    fig, axes = plt.subplots(1, len(model_test.columns), figsize=(5*len(model_test.columns), 5))\n",
    "    axes = axes.flatten()\n",
    "    for ipredictor, predictor in enumerate(model_test.columns):\n",
    "        # print(f\"predictor {predictor}\")\n",
    "            \n",
    "        axes[ipredictor].plot(model_test.sort_index()[predictor])\n",
    "        axes[ipredictor].set_title(predictor)\n",
    "        axes[ipredictor].grid(True)\n",
    "        \n",
    "        tup_y_lim = (min(tup_y_lim[0], model_test[predictor].min()-0.1),\n",
    "                    max(tup_y_lim[1], model_test[predictor].max()+0.1))\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_ylim(tup_y_lim)\n",
    "    fig.suptitle('GLM Coefficients Fit Results')\n",
    "    plt.show()\n",
    "    plt.savefig(save_img + '/results/model_fit.png')\n",
    "    print(f\"saved to: {save_img + '/results/model_fit.png'}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
