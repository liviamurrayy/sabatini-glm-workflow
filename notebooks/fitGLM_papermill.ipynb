{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os \n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    os.chdir('..')\n",
    "import glob\n",
    "\n",
    "import sys\n",
    "print(sys.path)\n",
    "\n",
    "from sglm import utils, glm_fit\n",
    "\n",
    "#d1 - T430\n",
    "#d2 - T434"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, let's create a new project. The project directory will create a data and results folder and a config file.\n",
    "\n",
    "#### You will need to edit the config file with the particular glm params you wish to use. Fields that are necessary to edit are: predictors, predictors_shift_bounds, response, and the glm_keyword_args.\n",
    "\n",
    "#### You will also need to move your data into the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'T430_newNoShuffle_glm' #'D1_h3Hist_all_glm' \n",
    "project_dir = r'\\\\research.files.med.harvard.edu\\\\Neurobio\\\\MICROSCOPE\\\\Livia\\\\glm_output' # windows\n",
    "# project_dir = r'/Volumes/Neurobio/MICROSCOPE/Livia/glm_output' # mac\n",
    "\n",
    "utils.create_new_project(project_name, project_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Format Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input data should conform to the following convention and be saved as a *.csv:\n",
    "\n",
    "Indices / Unique Row Identifiers:\n",
    "* SessionName -- Any order is acceptable\n",
    "* TrialNumber-- Must be in chronological order, but does not need to start from zero\n",
    "* Timestamp -- Must be in chronological order, but does not need to start from zero\n",
    "\n",
    "Columns (Predictors + Responses):\n",
    "* Predictors - binary\n",
    "* Reponses - e.g. neural responses (analog or binary)\n",
    "\n",
    "Example, shown below is dummy data depicting a trial_0 that last four response timestamps:\n",
    "| SessionName | TrialNumber | Timestamp | predictor_1 | predictor_2 | predictor_3 | response_1 | response_2 |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| session_0 | trial_0 | -1 | 0 | 0 | 0 | 1 | 0.3 |\n",
    "| session_0 | trial_0 | 0 | 0 | 0 | 0 | 0 | 1.4 |\n",
    "| session_0 | trial_0 | 1 | 0 | 0 | 0 | 1 | 2.3 |\n",
    "| session_0 | trial_0 | 2 | 0 | 1 | 0 | 1 | 0.3 |\n",
    "| session_0 | trial_1 | -2 | 0 | 0 | 0 | 0 | 1.4 |\n",
    "| session_0 | trial_1 | -1 | 0 | 0 | 0 | 1 | 2.3 |\n",
    "| session_0 | trial_1 | 0 | 1 | 0 | 0 | 0 | 1.4 |\n",
    "| session_0 | trial_1 | 1 | 0 | 0 | 0 | 1 | 2.3 |\n",
    "| session_1 | trial_0 | 5 | 0 | 0 | 0 | 0 | 1.4 |\n",
    "| session_1 | trial_0 | 6 | 1 | 0 | 0 | 1 | 2.3 |\n",
    "| session_1 | trial_0 | 7 | 0 | 0 | 0 | 0 | 1.4 |\n",
    "| session_1 | trial_0 | 8 | 0 | 0 | 0 | 1 | 2.3 |\n",
    "| session_1 | trial_1 | 9 | 0 | 0 | 0 | 0 | 1.4 |\n",
    "| session_1 | trial_1 | 10 | 0 | 0 | 0 | 1 | 2.3 |\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's get set up to start our project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = os.path.join(project_dir, project_name)\n",
    "files = os.listdir(project_path)\n",
    "\n",
    "assert 'data' in files, 'data folder not found! {}'.format(files)\n",
    "assert 'results' in files, 'results folder not found! {}'.format(files)\n",
    "assert 'config.yaml' in files, 'config.yaml not found! {}'.format(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If needed, use the following function to combine multiple sessions into one csv. You will need a filename you wish to call your output_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = False\n",
    "if left:\n",
    "    # output_csv = 'output.csv'\n",
    "    p_split = project_name.split(\"_\")\n",
    "    p_name = p_split[0][:len(p_split[0])-1]\n",
    "    name = p_name +\"_\"+ p_split[1]\n",
    "    print(name)\n",
    "    output_csv = name +'Format.csv'\n",
    "else: output_csv = project_name +'Format.csv'\n",
    "\n",
    "output_csv\n",
    "# utils.combine_csvs(project_path, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we'll open the data and set the columns you wish to use as fixed indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = os.path.join(project_path, 'data', output_csv)\n",
    "index_col = ['SessionName', 'TrialNumber', 'Timestamp']\n",
    "\n",
    "df = utils.read_data(input_file, index_col)\n",
    "\n",
    "print('Your dataframe has {} rows and {} columns'.format(df.shape[0], df.shape[1]))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can now explore and add to the dataframe. As an example, you may want to add various \"predictors\" or \"features\" to explore. You can use the example below as inspiration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Identify the individual licks that have specific meaning in the tasks: \n",
    "# #lick 1, lick 2 and lick 3 are \"operant licks\" on different training days\n",
    "# #licknon1-3 are all the other licks\n",
    "\n",
    "# df_source = df.copy()\n",
    "# srs_lick = df_source.groupby(['SessionName', 'TrialNumber'])['Lick'].cumsum()\n",
    "# srs_lick_count = srs_lick * df_source['Lick']\n",
    "# df_lick_count_dummies = pd.get_dummies(srs_lick_count).drop(0, axis=1)\n",
    "# df_lick_count_dummies = df_lick_count_dummies[[1,2,3]]\n",
    "# df_lick_count_dummies['non1-3'] = df_source['Lick'] - df_lick_count_dummies.sum(axis=1)\n",
    "# df_lick_count_dummies.columns = [f'lick_{original_column_name}' for original_column_name in df_lick_count_dummies.columns]\n",
    "\n",
    "# # Columns lick and lick_1, lick_2, lick_3, lick_non-13 should not all be used together\n",
    "# # as predictors because of multicollinearity.\n",
    "# df_source = pd.concat([df_source, df_lick_count_dummies], axis=1)\n",
    "# df_source\n",
    "\n",
    "# assert np.all(df_source['Lick'] == df_source[['lick_1', 'lick_2', 'lick_3', 'lick_non1-3']].sum(axis=1)), 'Column lick should equal the sum of all other lick columns.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Friendly reminder, the df we have imported is mutli-index, meaning, it's organization is dependent on 3-columns that we have set in index_col. Therefore, we can use \"groupby\" if you need to split the organization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reIndex = df_source.groupby(level=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load your fitting paramaters and set up your train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = os.path.join(project_path, 'config.yaml')\n",
    "config = utils.load_config(config_file)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shift responses and predictors. If you do not want to shift your predictors by an amount you set, feel free to comment out the entire \"predictors_shift_bounds\" in config.yaml. We will then use the default set when we created the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(col, df[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_shift, df_predictors_shift, shifted_params = glm_fit.shift_predictors(config, df)\n",
    "print('Your dataframe was shifted using: {}'.format(shifted_params))\n",
    "\n",
    "df_predictors_shift.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictors_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enl_columns = df_predictors_shift.filter(like='enl', axis=1)\n",
    "session = 'T430_2023_08_14'\n",
    "filt_ind = [ind for ind, col in enumerate(enl_columns.index) if session in col]\n",
    "\n",
    "rows_taken = [col for col in enl_columns.index if session in col]\n",
    "print(F\"{session} rows num: {len(rows_taken)}\")\n",
    "\n",
    "combined_all = enl_columns.iloc[filt_ind].sum(axis=1)\n",
    "for com in combined_all.unique():\n",
    "    print(f\"{com} number: {len(np.where(combined_all == com)[0])}\")\n",
    "combined_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{0} number: {np.where(combined_all == 0)[0]}\")\n",
    "\n",
    "test = enl_columns.iloc[80:100]\n",
    "# test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create sparse array - by row\n",
    "# import scipy\n",
    "# sparse_df = scipy.sparse.csr_array(df_predictors_shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your test/train datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## X_train,X_test, y_train, y_test = glm_fit.split_data(df_predictors_shift, temp, config) \n",
    "# X_train,X_test, y_train, y_test = glm_fit.split_data(sparse_df, temp, config) \n",
    "\n",
    "# import trial_split\n",
    "\n",
    "def hist(set):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Create a histogram\n",
    "    plt.hist(set, bins=len(np.unique(set)), color='blue', alpha=0.7)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def renumber_trials(trials):\n",
    "    \n",
    "    arr = trials.copy()       \n",
    "    found_ind = []\n",
    "    count = 0\n",
    "    new_tn = []\n",
    "\n",
    "    unique_values, ind = np.unique(arr, axis=0, return_index=True)\n",
    "    for i in ind:\n",
    "        val_ind = np.where(np.all(arr == arr[i, :], axis=1))[0]\n",
    "        found_ind.extend(val_ind)\n",
    "        new_tn.extend(np.full_like(val_ind, count)) \n",
    "        count += 1\n",
    "    \n",
    "    new_tn = np.array(new_tn)\n",
    "    return new_tn\n",
    "\n",
    "'''\n",
    "    for i in range(len(arr)):\n",
    "    \n",
    "        if i == 0:\n",
    "            # print(f\"first trial: {count}\")\n",
    "            # print(f\"session, trial#: {arr[i, :]}\")\n",
    "            \n",
    "            # print(f\"found arr: {found}\")\n",
    "            val_ind = np.where(np.all(arr == arr[i, :], axis=1))[0]\n",
    "            found_ind.extend(val_ind)\n",
    "            # print(f\"found indexes: {found_ind}\")\n",
    "            new_tn.extend(np.full_like(val_ind, count)) \n",
    "            # print(f\"trial numbering: {new_tn}\")\n",
    "        else:\n",
    "            if any(np.array_equal(i, found_elem) for found_elem in found_ind):\n",
    "                continue\n",
    "            else:\n",
    "                count += 1\n",
    "                # print(f\"new trial: {count}\")\n",
    "                # print(f\"session, trial#: {arr[i, :]}\")\n",
    "                # print(f\"found new session: {count}\")          \n",
    "                \n",
    "                # print(f\"found arr: {found}\")\n",
    "                val_ind = np.where(np.all(arr == arr[i, :] , axis=1))[0]\n",
    "                found_ind.extend(val_ind)\n",
    "                # print(f\"found indexes: {found_ind}\")\n",
    "                new_tn.extend(np.full_like(val_ind, count)) \n",
    "                # print(f\"trial numbering: {new_tn}\")\n",
    "                    \n",
    "    found_ind = np.array(found_ind)\n",
    "    new_tn = np.array(new_tn)\n",
    "    \n",
    "    # print(f\"final found indexes: {found_ind}\")\n",
    "    if np.array_equal(found_ind, np.arange(len(trials))):\n",
    "        # print(f\"final new trial numbering: {new_tn}\")\n",
    "        return new_tn\n",
    "    else: \n",
    "        print(\"ERROR not all indexes reassigned trial number\")\n",
    "        return np.nan\n",
    "'''\n",
    "\n",
    "# test = np.vstack((np.array(['a', 'a', 'b', 'c', 'd', 'e', 'f']).astype(str), np.array([1, 1, 2, 2, 3, 4, 1]).astype(float))).transpose()\n",
    "# print(test)  \n",
    "# new = renumber_trials(test)    \n",
    "# new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trialsss = (np.vstack((np.array(df_predictors_shift.index.get_level_values(0)).astype(str), df_predictors_shift.index.get_level_values(1)))).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split_trials(config, df, y):\n",
    "    \n",
    "    trials = (np.vstack((np.array(df.index.get_level_values(0)).astype(str), df.index.get_level_values(1)))).transpose()\n",
    "    # print(trials)\n",
    " \n",
    "    # new_trial_numbering = renumber_trials(trials)\n",
    "    # mapping = np.vstack((df.index.get_level_values(1), new_trial_numbering)).transpose()\n",
    "    \n",
    "    ind = {\n",
    "    'session': np.array(df.index.get_level_values(0)).astype(str),\n",
    "    'trial_num_new': df.index.get_level_values(1) # new_trial_numbering\n",
    "    }\n",
    "    arrays = list(zip(ind['session'], ind['trial_num_new']))\n",
    "    multi_index = pd.MultiIndex.from_tuples(arrays, names=('session', 'trial_num_new'))\n",
    "    df.set_index(multi_index, inplace=True)\n",
    "    \n",
    "    train_size = config['train_test_split']['train_size']\n",
    "    test_size = config['train_test_split']['test_size']\n",
    "    \n",
    "    print(train_size, test_size)\n",
    "    \n",
    "    # new_trials = (np.vstack((np.array(df.index.get_level_values(0)).astype(str), df.index.get_level_values(1)))).transpose()\n",
    "\n",
    "    # print(new_trials[:, 1])\n",
    "    # hist(new_trials[:, 1])\n",
    "    \n",
    "    # unique_values, unique_ind, unique_count = np.unique(new_trials[:, 1], return_index=True, return_counts=True)\n",
    "    unique_values, unique_ind, unique_count = np.unique(trials[:, 1], return_index=True, return_counts=True)\n",
    "\n",
    "    print(f\"unique val: {unique_values}\")\n",
    "    print(f\"unique ind: {unique_ind}\")\n",
    "    # hist(unique_values)\n",
    "    print(f\"unique count {unique_count}\")\n",
    "    \n",
    "    sample_size = int(test_size * len(unique_values))\n",
    "    test_sample = np.random.choice(unique_values, size=sample_size, replace=False)\n",
    "    # print(f\"testsample: {test_sample}\")\n",
    "    \n",
    "    print(f\"test:train ratio: {len(test_sample) / len(unique_values)}:{(len(unique_values)-len(test_sample))/len(unique_values)}\")\n",
    "    \n",
    "    ind = []\n",
    "    for test in test_sample:\n",
    "        val_ind = np.where(trials[:, 1] == test)[0]\n",
    "        ind.extend(val_ind)\n",
    "        \n",
    "    print(f\"ind {ind}\")\n",
    "    \n",
    "    copy = df_predictors_shift.copy()\n",
    "    copy = copy.reset_index(drop=True)\n",
    "    X_test = copy.loc[ind, :]\n",
    "    X_train = copy.drop(index=ind)\n",
    "    y_test = y[ind]\n",
    "    y_train = np.delete(y, ind)\n",
    "    test_ind = df.index.get_level_values(1)[ind] #new_trial_numbering[ind]\n",
    "    \n",
    "    return test_ind, X_train, X_test, y_train, y_test #X_train,X_test, y_train, y_test\n",
    "\n",
    "# X_train,X_test, y_train, y_test = trial_split(config, df, temp)\n",
    "test_list, X_train,X_test, y_train, y_test = test_train_split_trials(config, df_predictors_shift, response_shift.values.flatten())\n",
    "\n",
    "# print('Training data has {} rows and {} columns'.format(X_train.shape[0], X_train.shape[1]))\n",
    "# print('Testing data has {} rows and {} columns'.format(X_test.shape[0], X_test.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_test)/len(y_train), len(X_test)/len(X_train))\n",
    "y_test.shape, y_train.shape, X_test.shape, X_train.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "346930x2526 sparse array of type '<class 'numpy.float64'>'\n",
    "\twith 3242262 stored elements in Compressed Sparse Row format>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "# y_train_sp = scipy.sparse.csr_array(y_train)\n",
    "# y_test_sp = scipy.sparse.csr_array(y_test)\n",
    "x_train_sp = scipy.sparse.csr_array(X_train)\n",
    "x_test_sp = scipy.sparse.csr_array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_sp, x_test_sp #, y_test_sp, y_train_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test1 = (y_test.values).flatten()\n",
    "X_test.shape, y_test.shape, X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we're ready to run our GLM!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have two different options. If you know which params you would like to use, you can use the glm_fit.fit_glm function. If you would like tune your hyperparams to determine which are the best to use, you can use the glm_fit.fit_tuned_glm function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit the model\n",
    "# model, y_pred, score, beta, intercept, sparse_beta = glm_fit.fit_glm(config, x_train_sp, x_test_sp, y_train, y_test)\n",
    "# print('Your model can account for {} percent of your data'.format(score*100))\n",
    "# modeltype = \"fit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Model:\n",
    "# model, y_pred, score, beta, intercept = glm_fit.fit_ridge(config, x_train_sp, x_test_sp, y_train, y_test)\n",
    "# print('Your model can account for {} percent of your data'.format(score*100))\n",
    "# modeltype = \"fit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Model CV:\n",
    "# tuned_model, y_pred, score, beta, best_params = glm_fit.fit_tuned_ridge(config, x_train_sp, x_test_sp, y_train, y_test)\n",
    "# print('Your model can account for {} percent of your data'.format(score*100))\n",
    "# modeltype = \"tuned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_plot(config, df):\n",
    "    shifts = config['glm_params']['predictors_shift_bounds']\n",
    "    # print(shifts)\n",
    "    # col_list = []\n",
    "    # index = []\n",
    "    # len_shift = 0\n",
    "    plot_df = pd.DataFrame()\n",
    "    for key, value in shifts.items():\n",
    "        # print(f\"key = {key}\")\n",
    "        if key == 'z_grn':\n",
    "            continue\n",
    "        else:\n",
    "            plot_df[key] = df[key]['0']\n",
    "            '''\n",
    "            # col_list.append(key)\n",
    "            # # print(value, value[0], value[1])\n",
    "            # head = value[0]\n",
    "            # tail = value[1]\n",
    "            # index.append(abs(head)+len_shift)\n",
    "            # len_shift = abs(head)+abs(tail)+len_shift\n",
    "            # # print(f\"len sh = {len_shift}\")\n",
    "            '''\n",
    "    # print(index)\n",
    "            \n",
    "    # plot_pd = pd.DataFrame(df.iloc[:, index], columns=col_list)\n",
    "    \n",
    "    return plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = prep_plot(config, X_test)\n",
    "plot_df = plot_df.set_index(test_list, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model with cross validation: remember, your alphas and l1_ratios should be lists\n",
    "tuned_model, y_pred, score, beta, best_params = glm_fit.fit_tuned_glm(config, X_train, X_test, y_train, y_test)\n",
    "print('Your model can account for {} percent of your data'.format(score*100))\n",
    "modeltype = 'tuned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, window):\n",
    "    \n",
    "    r = x.rolling(window=window,center=True)\n",
    "    \n",
    "    lower_bound = r.min()\n",
    "    upper_bound = r.max()\n",
    "    #lower_bound = r.quantile(0.05) # instead of .min()\n",
    "    #upper_bound = r.quantile(0.95) # instead of .max()\n",
    "    \n",
    "    return (x-lower_bound)/(upper_bound-lower_bound)\n",
    "  \n",
    "def plot_spikes_with_prediction(df, spikes, predicted_spikes, fs, **kws):\n",
    "  \"\"\"Plot actual and predicted spike counts.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  f, ax = plt.subplots(2, figsize=(12, 25))\n",
    "  # lines = ax.stem(t, spikes[:nt])\n",
    "  # plt.setp(lines, color=\".5\")\n",
    "  \n",
    "  i=0\n",
    "  inc_height=1.2\n",
    "  window_start = 0  #10200 \n",
    "  window_stop = len(spikes) #10500\n",
    "\n",
    "  cols_to_plot = [col for col in df.columns]\n",
    "  enl_columns = [col for col in df.columns if 'enl' in col]\n",
    "  not_enl = [col for col in cols_to_plot if not any(enl_col in col for enl_col in enl_columns)]\n",
    "  \n",
    "  for j, col in enumerate(not_enl):\n",
    "    if 'trial_num' in col:\n",
    "      continue\n",
    "    if 'z_grn' in col:\n",
    "      ax[0].plot((df[col].values*0.2)[window_start:window_stop] + i, label=col)\n",
    "    else:\n",
    "      ax[0].plot((df[col].values)[window_start:window_stop] + i, label=col)\n",
    "    i += inc_height\n",
    "    ax[0].text(x=len(spikes[window_start:window_stop])+len(spikes[window_start:window_stop])*0.1, y=i - inc_height, s=col, fontsize=14, va=\"bottom\")\n",
    "  ax[0].set(\n",
    "      xlabel=\"Bins\",\n",
    "      ylabel=\"Events\",\n",
    "      title=f\"Photometry signal reconstruction: {project_name}\"\n",
    "  )\n",
    "  \n",
    "  t = (np.arange(0, len(spikes))*fs)[window_start:window_stop]\n",
    "\n",
    "#_____________________\n",
    "  trials = plot_df.index[window_start:window_stop]\n",
    "  trial_call = trials.unique()\n",
    "  print(f\"trials: {trial_call}\")\n",
    "\n",
    "  for tt, trial in enumerate(trial_call):\n",
    "    if tt%2 != 0:\n",
    "      # print(t[np.where(trials == trial)[0][1]], t[np.where(trials == trial)[0][-1]])\n",
    "      ax[1].axvspan(xmin=t[(np.where(trials == trial)[0])[0]], xmax=t[(np.where(trials == trial)[0])[-1]], ymin=-2, ymax=2, color='lightgrey', alpha=0.2)\n",
    "  #____________________\n",
    "  ax[1].plot(t, spikes[window_start:window_stop], label='real Y')\n",
    "  # lines[-1].set_zorder(1)\n",
    "  kws.setdefault(\"linewidth\", 3)\n",
    "  yhat, = ax[1].plot(t, predicted_spikes[window_start:window_stop], **kws, label='pred Y')\n",
    "  ax[1].set(\n",
    "      xlabel=\"Seconds\",\n",
    "      ylabel=\"Photometry\",\n",
    "  )\n",
    "  ax[1].yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "  ax[1].axhline(y=0, color='grey', linestyle=\"--\")\n",
    "  # ax.legend([lines[0], yhat], [\"Spikes\", \"Predicted\"])\n",
    "  ax[1].legend()\n",
    "\n",
    "  plt.show()\n",
    "  plt.savefig(os.path.join(project_dir, project_name) + '/results/beh_reconstructions.png')\n",
    "  \n",
    "fs = 1/48.440600198412696\n",
    "\n",
    "input_df = prep_plot(config, X_test) #, test_list)\n",
    "# plot_spikes_with_prediction(y_test, y_pred, df.Timestamp[1] - df.Timestamp[0])\n",
    "plot_spikes_with_prediction(input_df, y_test, y_pred, fs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tri = \n",
    "# # print(mapping(np.where(mapping[1] == tri)[0]))\n",
    "# test = plot_df[plot_df.index == tri]\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, ax = plt.subplots(1, figsize=(6, 12))\n",
    "  \n",
    "# i=0\n",
    "# inc_height=1.2\n",
    "\n",
    "# cols_to_plot = [col for col in test.columns]\n",
    "\n",
    "# for j, col in enumerate(cols_to_plot):\n",
    "#     ax.plot((test[col].values) + i, label=col)\n",
    "#     i += inc_height\n",
    "#     ax.text(x=10, y=i - inc_height, s=col, fontsize=14, va=\"bottom\")\n",
    "#     ax.set(\n",
    "#         xlabel=\"Bins\",\n",
    "#         ylabel=\"Events\",\n",
    "#         title=f\"Photometry signal reconstruction: {project_name}\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modeltype == 'tuned':\n",
    "    print(best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create your model dictonary, this should include all the information you wish to save\n",
    "if modeltype == \"fit\":\n",
    "    model_dict = {'model': model,\n",
    "              'y_pred': y_pred,\n",
    "              'score': score,\n",
    "              'beta': beta,\n",
    "              'intercept': intercept,\n",
    "              'sparse_beta': sparse_beta,}\n",
    "if modeltype == 'tuned':\n",
    "    model_dict = {'model': tuned_model,\n",
    "              'y_pred': y_pred,\n",
    "              'score': score,\n",
    "              'beta': beta,\n",
    "              'best_params': best_params,}\n",
    "    \n",
    "\n",
    "#Save your model dictionary\n",
    "model_path = project_path + '/models'\n",
    "model_name = project_name + '_model.pkl'\n",
    "model_full_path = os.path.join(model_path, model_name)\n",
    "import pickle\n",
    "with open(model_full_path, 'wb') as f:\n",
    "    pickle.dump(model_dict, f)\n",
    "        \n",
    "# glm_fit.save_model(model_dict, project_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and save figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_img = os.path.join(project_path)\n",
    "\n",
    "glm_fit.plot_and_save(save_img, config, y_pred, y_test, beta, df_predictors_shift, format(score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_enl_forwards = False\n",
    "combine_enl_backwards = True\n",
    "if combine_enl_forwards:\n",
    "    #save model fit results\n",
    "    model_fit_results = pd.Series(beta, index=df_predictors_shift.columns, name='coef').unstack(0, )\n",
    "    model_fit_results.index = model_fit_results.index.astype(int)\n",
    "    model_fit_results = model_fit_results.reindex(config['glm_params']['predictors'], axis=1)\n",
    "\n",
    "    # new added by liv for enl appending:\n",
    "    if any('enl' in col for col in model_fit_results.columns):\n",
    "        # could add sorting for enl precursors ie. history?\n",
    "        enl_columns = [col for col in model_fit_results.columns if 'enl' in col]\n",
    "        enl_col_df = model_fit_results[enl_columns]\n",
    "        # print(enl_col_df.shape)\n",
    "        # for icol, col in enumerate(enl_col_df.columns):\n",
    "            # print(\"col shape: \" + col.shape)\n",
    "            # print(f\"col ind: {icol}\")\n",
    "            # print(f\"col index pred shape: {enl_col_df.sort_index()[col]}\")\n",
    "        print(\"test - true: enl found\")\n",
    "            \n",
    "        enl_all = np.array([])\n",
    "        test = []\n",
    "        for icol, col in enumerate(enl_col_df.columns):\n",
    "            test.append(enl_col_df.sort_index()[col])\n",
    "            nans = np.isnan(enl_col_df.sort_index()[col])\n",
    "            enl_all = np.append(enl_all, enl_col_df.sort_index()[col][~nans])\n",
    "            \n",
    "        print(f\"enl all shape: {enl_all.shape}\")\n",
    "        enl_df = pd.DataFrame({'enl': enl_all})\n",
    "        \n",
    "        test = np.where(enl_df['enl'] == 0)[0]\n",
    "        model_test = model_fit_results.copy()\n",
    "        # print(model_test.index)\n",
    "        loc_insert = int((np.where(model_test.columns == enl_columns[0])[0])[0])\n",
    "        \n",
    "        model_test = model_test.sort_index()\n",
    "        model_test = model_test.reindex(range(np.min(model_test.index), len(enl_all)))\n",
    "        print(f\"model test shape: {model_test.shape}\")\n",
    "            \n",
    "        for col in enl_columns:\n",
    "            # print(f\"drop col: {str(col)}\")\n",
    "            model_test = model_test.drop(str(col), axis=1)\n",
    "\n",
    "        # print(model_test.index)\n",
    "        model_test = model_test.join(enl_df, how='left')\n",
    "        col = model_test.pop('enl')\n",
    "        model_test.insert(loc_insert, col.name, col)\n",
    "\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print(\"no enl found\")\n",
    "\n",
    "    ''' uncomment for comparison with and without joining enl:\n",
    "    tup_y_lim = (np.inf, -np.inf)\n",
    "    fig, axes = plt.subplots(1, len(model_fit_results.columns), figsize=(5*len(model_fit_results.columns), 5))\n",
    "    axes = axes.flatten()\n",
    "    for ipredictor, predictor in enumerate(model_fit_results.columns):\n",
    "            \n",
    "        axes[ipredictor].plot(model_fit_results.sort_index()[predictor])\n",
    "        axes[ipredictor].set_title(predictor)\n",
    "        axes[ipredictor].grid(True)\n",
    "        \n",
    "        tup_y_lim = (min(tup_y_lim[0], model_fit_results[predictor].min()-0.1),\n",
    "                    max(tup_y_lim[1], model_fit_results[predictor].max()+0.1))\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_ylim(tup_y_lim)\n",
    "    fig.suptitle('GLM Coefficients Fit Results')\n",
    "    '''\n",
    "    ###########\n",
    "    tup_y_lim = (np.inf, -np.inf)\n",
    "    fig, axes = plt.subplots(1, len(model_test.columns), figsize=(5*len(model_test.columns), 5))\n",
    "    axes = axes.flatten()\n",
    "    for ipredictor, predictor in enumerate(model_test.columns):\n",
    "        # print(f\"predictor {predictor}\")\n",
    "            \n",
    "        axes[ipredictor].plot(model_test.sort_index()[predictor])\n",
    "        axes[ipredictor].set_title(predictor)\n",
    "        axes[ipredictor].grid(True)\n",
    "        \n",
    "        tup_y_lim = (min(tup_y_lim[0], model_test[predictor].min()-0.1),\n",
    "                    max(tup_y_lim[1], model_test[predictor].max()+0.1))\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_ylim(tup_y_lim)\n",
    "    fig.suptitle('GLM Coefficients Fit Results')\n",
    "    plt.savefig(save_img + '/results/model_fit_comb.png')\n",
    "    print(f\"saved to: {save_img + '/results/model_fit_comb.png'}\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "if combine_enl_backwards:\n",
    "    #save model fit results\n",
    "    model_fit_results = pd.Series(beta, index=df_predictors_shift.columns, name='coef').unstack(0, )\n",
    "    model_fit_results.index = model_fit_results.index.astype(int)\n",
    "    model_fit_results = model_fit_results.reindex(config['glm_params']['predictors'], axis=1)\n",
    "\n",
    "    # new added by liv for enl appending:\n",
    "    if any('enl' in col for col in model_fit_results.columns):\n",
    "        # could add sorting for enl precursors ie. history?\n",
    "        enl_columns = [col for col in model_fit_results.columns if 'enl' in col]\n",
    "        enl_col_df = model_fit_results[enl_columns]\n",
    "        # print(enl_col_df.shape)\n",
    "        # for icol, col in enumerate(enl_col_df.columns):\n",
    "            # print(\"col shape: \" + col.shape)\n",
    "            # print(f\"col ind: {icol}\")\n",
    "            # print(f\"col index pred shape: {enl_col_df.sort_index()[col]}\")\n",
    "        print(\"test - true: enl found\")\n",
    "            \n",
    "        # enl_all = np.array([])\n",
    "        enl_all = enl_col_df.mean(axis=1, skipna=True)\n",
    "        # test = []\n",
    "        # for icol, col in enumerate(enl_col_df.columns):\n",
    "        #     test.append(enl_col_df.sort_index()[col])\n",
    "        #     nans = np.isnan(enl_col_df.sort_index()[col])\n",
    "        #     enl_all = np.append(enl_all, enl_col_df.sort_index()[col][~nans])\n",
    "            \n",
    "        print(f\"enl all shape: {enl_all.shape}\")\n",
    "        enl_df = pd.DataFrame({'enl': enl_all})\n",
    "        \n",
    "        test = np.where(enl_df['enl'] == 0)[0]\n",
    "        model_test = model_fit_results.copy()\n",
    "        # print(model_test.index)\n",
    "        loc_insert = int((np.where(model_test.columns == enl_columns[0])[0])[0])\n",
    "        \n",
    "        model_test = model_test.sort_index()\n",
    "        model_test = model_test.reindex(range(np.min(model_test.index), len(enl_all)))\n",
    "        print(f\"model test shape: {model_test.shape}\")\n",
    "            \n",
    "        for col in enl_columns:\n",
    "            # print(f\"drop col: {str(col)}\")\n",
    "            model_test = model_test.drop(str(col), axis=1)\n",
    "\n",
    "        # print(model_test.index)\n",
    "        model_test = model_test.join(enl_df, how='left')\n",
    "        col = model_test.pop('enl')\n",
    "        model_test.insert(loc_insert, col.name, col)\n",
    "\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print(\"no enl found\")\n",
    "\n",
    "    ''' uncomment for comparison with and without joining enl:\n",
    "    tup_y_lim = (np.inf, -np.inf)\n",
    "    fig, axes = plt.subplots(1, len(model_fit_results.columns), figsize=(5*len(model_fit_results.columns), 5))\n",
    "    axes = axes.flatten()\n",
    "    for ipredictor, predictor in enumerate(model_fit_results.columns):\n",
    "            \n",
    "        axes[ipredictor].plot(model_fit_results.sort_index()[predictor])\n",
    "        axes[ipredictor].set_title(predictor)\n",
    "        axes[ipredictor].grid(True)\n",
    "        \n",
    "        tup_y_lim = (min(tup_y_lim[0], model_fit_results[predictor].min()-0.1),\n",
    "                    max(tup_y_lim[1], model_fit_results[predictor].max()+0.1))\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_ylim(tup_y_lim)\n",
    "    fig.suptitle('GLM Coefficients Fit Results')\n",
    "    '''\n",
    "    ###########\n",
    "    tup_y_lim = (np.inf, -np.inf)\n",
    "    fig, axes = plt.subplots(1, len(model_test.columns), figsize=(5*len(model_test.columns), 5))\n",
    "    axes = axes.flatten()\n",
    "    for ipredictor, predictor in enumerate(model_test.columns):\n",
    "        # print(f\"predictor {predictor}\")\n",
    "            \n",
    "        axes[ipredictor].plot(model_test.sort_index()[predictor])\n",
    "        axes[ipredictor].set_title(predictor)\n",
    "        axes[ipredictor].grid(True)\n",
    "        \n",
    "        tup_y_lim = (min(tup_y_lim[0], model_test[predictor].min()-0.1),\n",
    "                    max(tup_y_lim[1], model_test[predictor].max()+0.1))\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_ylim(tup_y_lim)\n",
    "    fig.suptitle('GLM Coefficients Fit Results')\n",
    "    plt.savefig(save_img + '/results/model_fit_comb.png')\n",
    "    print(f\"saved to: {save_img + '/results/model_fit_comb.png'}\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame({\n",
    "    'y_pred': y_pred,\n",
    "    'y_real': y_test\n",
    "})\n",
    "y.to_csv(\"//research.files.med.harvard.edu/Neurobio/MICROSCOPE/Livia/glm_output/y.csv\")\n",
    "plot_df.to_csv(\"//research.files.med.harvard.edu/Neurobio/MICROSCOPE/Livia/glm_output/df.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_av_coef(config, df(X_test), enl, y):\n",
    "def first_cue(trial, cues):\n",
    "    \n",
    "    cuetype = None\n",
    "    for cue in cues:\n",
    "        # print(cue)\n",
    "        # print(np.where(trial[cue] == 1)[0])\n",
    "        if len(np.where(trial[cue] == 1)[0]) > 0:\n",
    "            cuetype = str(cue)\n",
    "            time = np.where(trial[cue] == 1)[0][0]\n",
    "            break\n",
    "          \n",
    "    if cuetype is None:\n",
    "        print(\"no cue found - big problem\")\n",
    "        return cuetype, 0\n",
    "    else:\n",
    "        return cuetype, time\n",
    "    \n",
    "    \n",
    "cols_to_plot = [col for col in plot_df.columns]\n",
    "enl_columns = [col for col in plot_df.columns if 'enl' in col]\n",
    "not_more = [col for col in plot_df.columns if not 'more' in col]\n",
    "cues = [col for col in plot_df.columns if 'cue' in col]\n",
    "not_ = [col for col in not_more if not 'enl' in col]\n",
    "print(not_)\n",
    "\n",
    "u, count = np.unique(plot_df.index, return_counts=True)\n",
    "length = np.max(count)\n",
    "averaged_event_df = pd.DataFrame()\n",
    "real_event_df = pd.DataFrame()\n",
    "    \n",
    "for trial_num in plot_df.index.unique():\n",
    "    trial = plot_df[plot_df.index == trial_num]\n",
    "    y_trial = y_pred[plot_df.index == trial_num]\n",
    "    y_real = y_test[plot_df.index == trial_num]\n",
    "    # print(y_trial.shape, trial.shape)\n",
    "    \n",
    "    nones = []\n",
    "    for col in not_:    \n",
    "        if np.any((np.where(trial[col] == 1))[0]):\n",
    "            cue_type, first_cue_ind = first_cue(trial, cues)\n",
    "            if cue_type is None:\n",
    "                nones.append([trial_num, col])\n",
    "                #is this enl are they their own trials??\n",
    "            # print(f\"{cue_type}: {first_cue_ind}\")\n",
    "            \n",
    "            event_new = y_trial[first_cue_ind:len(trial[col])] #HERE\n",
    "            event_real = y_real[first_cue_ind:len(trial[col])]\n",
    "            \n",
    "            if col in averaged_event_df:\n",
    "                a = averaged_event_df[col].values\n",
    "                b = event_new\n",
    "                b_nans = np.append(event_new, np.full(abs(len(b) - len(a)), np.nan))\n",
    "                    \n",
    "                mean_result = np.nanmean([a, b_nans], axis=0)  \n",
    "            else:\n",
    "                mean_result = event_new\n",
    "                \n",
    "            if col in real_event_df:\n",
    "                c = real_event_df[col].values\n",
    "                d = event_real\n",
    "                d_nans = np.append(event_real, np.full(abs(len(d) - len(c)), np.nan))\n",
    "                mean_real = np.nanmean([c, d_nans], axis=0)\n",
    "            else:\n",
    "                mean_real = event_real\n",
    "\n",
    "            mean_result = np.append(mean_result, np.full(abs(length - len(mean_result)), np.nan))\n",
    "            mean_real = np.append(mean_real, np.full(abs(length - len(mean_real)), np.nan))\n",
    "            \n",
    "\n",
    "            averaged_event_df[col] = mean_result  \n",
    "            real_event_df[col] = mean_real          \n",
    "            # print(f\"shapes: {col, averaged_event_df[col].shape, len(mean_result)}\")\n",
    "            \n",
    "    if len(nones) > 0:\n",
    "        print(f\"no cues?: {nones}\") \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(averaged_event_df.columns), figsize=(12, 25), sharex=True)\n",
    "print(len(averaged_event_df.columns))\n",
    "print(averaged_event_df.columns)\n",
    "\n",
    "for i, av in enumerate(np.sort(averaged_event_df.columns)): #cue select comsp: R, UR\n",
    "    t = np.arange(0, len(averaged_event_df[av])) / 48.440600198412696\n",
    "    t_real = np.arange(0, len(real_event_df[av])) / 48.440600198412696\n",
    "    ax[i].axhline(y=0, color='lightgrey')\n",
    "    ax[i].plot(t_real, real_event_df[av], label='real')\n",
    "    ax[i].plot(t, averaged_event_df[av], label='pred')\n",
    "    ax[i].set_ylabel(av, rotation = 45)\n",
    "    ax[i].axvline(x=0, color='k', linestyle=\"--\")\n",
    "    ax[i].legend()\n",
    "ax[0].set(\n",
    "    title=f\"Averaged signal reconstruction: {project_name}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(averaged_event_df.columns), figsize=(12, 25), sharex=True)\n",
    "print(len(averaged_event_df.columns))\n",
    "print(averaged_event_df.columns)\n",
    "\n",
    "for i, av in enumerate(np.sort(averaged_event_df.columns)):\n",
    "    ax[i].axhline(y=0, color='lightgrey')\n",
    "    ax[i].plot(real_event_df[av] - averaged_event_df[av], label='real - pred')\n",
    "    # ax[i].plot(averaged_event_df[av], label='pred')\n",
    "    ax[i].set_ylabel(av, rotation = 45)\n",
    "    ax[i].axvline(x=0, color='k', linestyle=\"--\")\n",
    "    ax[i].legend()\n",
    "ax[0].set(\n",
    "    title=f\"Averaged signal reconstruction: {project_name}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
